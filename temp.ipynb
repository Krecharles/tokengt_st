{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1], [2], [3]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Example input tensor (n x 9)\n",
    "x = torch.randint(0, 100, (5, 9))  # just sample data\n",
    "embedding_indices = x[:, 0].long()        # Shape: (n,)\n",
    "feature_inputs = x[:, 1:]                 # Shape: (n, 8)\n",
    "\n",
    "# Assume embedding layer\n",
    "embedding_layer = nn.Embedding(num_embeddings=100, embedding_dim=4)\n",
    "embedded = embedding_layer(embedding_indices)  # Shape: (n, 4)\n",
    "\n",
    "# Assume a simple feedforward for the features\n",
    "feature_layer = nn.Linear(8, 4)\n",
    "feature_processed = feature_layer(feature_inputs.float())  # Shape: (n, 4)\n",
    "\n",
    "# Concatenate both parts\n",
    "output = torch.cat([embedded, feature_processed], dim=1)  # Shape: (n, 8)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[18,  1, 15,  7, 89, 28, 55, 99,  0],\n",
       "        [49, 11, 98,  6, 33, 97, 93, 18, 23],\n",
       "        [19,  3, 84, 10, 65,  2, 63, 21, 75],\n",
       "        [47, 19, 82, 48,  3,  0,  4, 92, 23],\n",
       "        [74, 57, 26, 22, 26, 52, 80, 94, 76]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([150., 190.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Input tensor (n x 9)\n",
    "x = torch.tensor([\n",
    "    [0, 2, 3, 4, -1, 6, -1, -1, -1],\n",
    "    [1, 0, 1, -1, -1, -1, 5, 6, 7],\n",
    "], dtype=torch.long)\n",
    "\n",
    "embedding_indices = x[:, 0].long()        # Shape: (n,)\n",
    "indices = x[:, 1:]  # shape: (n, 8)\n",
    "\n",
    "# Assume embedding layer\n",
    "embedding_layer = nn.Embedding(num_embeddings=100, embedding_dim=4)\n",
    "embedded = embedding_layer(embedding_indices)  # Shape: (n, 4)\n",
    "\n",
    "\n",
    "value_map = torch.tensor([00, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100], dtype=torch.float)\n",
    "\n",
    "# Create mask: True where indices are not -1\n",
    "mask = indices != -1\n",
    "\n",
    "# Replace -1 with 0 (safe dummy index — will be masked out later)\n",
    "safe_indices = indices.clone()\n",
    "safe_indices[~mask] = 0  # now all -1s are 0\n",
    "\n",
    "# Lookup from value_map\n",
    "values = value_map[safe_indices]  # shape: (n, 8)\n",
    "\n",
    "# Zero out values where original index was -1\n",
    "values = values * mask.float()\n",
    "\n",
    "# Sum along dim=1\n",
    "row_sums = values.sum(dim=1)\n",
    "\n",
    "print(row_sums)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offset indices:\n",
      " tensor([[ 2,  3,  4, -1,  6, -1, -1, -1],\n",
      "        [ 8,  9,  7,  7,  7, 13, 14, 15]])\n",
      "Row sums:\n",
      " tensor([150., 590.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# x is shape [num_total_instances, 9]\n",
    "x = torch.tensor([\n",
    "    [0, 2, 3, 4, -1, 6, -1, -1, -1],\n",
    "    [1, 0, 1, -1, -1, -1, 5, 6, 7],\n",
    "], dtype=torch.long)\n",
    "\n",
    "embedding_indices = x[:, 0]       # (n_instances,)\n",
    "indices = x[:, 1:]                # (n_instances, max_substruct_size)\n",
    "\n",
    "# Value lookup (could be node-level features or external map)\n",
    "value_map = torch.tensor([0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150], dtype=torch.float)\n",
    "\n",
    "# n_substructure_instances per graph (2 instances total, across 2 graphs)\n",
    "n_substructure_instances = torch.tensor([1, 1])  # shape [batch_size]\n",
    "\n",
    "# ptr: cumulative node counts for batching (e.g., graph0 has 8 nodes, graph1 has 8)\n",
    "ptr = torch.tensor([0, 8, 16])  # shape [batch_size + 1]\n",
    "\n",
    "# Step 1: map each substructure instance to its graph\n",
    "instance_to_graph = torch.arange(len(n_substructure_instances)).repeat_interleave(n_substructure_instances)\n",
    "# -> tensor([0, 1])\n",
    "\n",
    "# Step 2: use ptr to offset node indices\n",
    "offsets = ptr[instance_to_graph]         # shape: [num_instances] → tensor([0, 8])\n",
    "offsets = offsets.unsqueeze(1)           # shape: [num_instances, 1] for broadcasting\n",
    "\n",
    "# Step 3: apply offset only where index != -1\n",
    "mask = indices != -1\n",
    "safe_indices = indices.clone()\n",
    "offset_indices = safe_indices + offsets  # now aligned to global node space\n",
    "safe_indices[~mask] = 0  # dummy index for -1s\n",
    "\n",
    "# Step 4: lookup and sum\n",
    "values = value_map[offset_indices]\n",
    "values = values * mask.float()\n",
    "row_sums = values.sum(dim=1)\n",
    "\n",
    "print(\"Offset indices:\\n\", offset_indices)\n",
    "print(\"Row sums:\\n\", row_sums)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "         1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 1, 2, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 1, 1, 2, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 6 # ie max key value, ie max(x[:, 0])\n",
    "v = 30 # ie max value possible in x[:, 1:]\n",
    "x = torch.tensor([\n",
    "    [ 2, 19, 24, 20, 23, 21, -1, -1, -1],\n",
    "    [ 3,  2,  3, 28,  4,  5, 27, -1, -1],\n",
    "    [ 3,  5, 27,  6, 10,  8,  7, -1, -1],\n",
    "    [ 3, 12, 26, 13, 25, 15, 14, -1, -1]\n",
    "], dtype=torch.long)\n",
    "keys = x[:, 0]         # shape (n,)\n",
    "indices = x[:, 1:]     # shape (n, 8)\n",
    "\n",
    "out = torch.zeros(n, v, dtype=torch.long)\n",
    "for i in range(x.size(0)):\n",
    "    key = x[i, 0]\n",
    "    valid_indices = x[i, 1:]\n",
    "    valid_indices = valid_indices[valid_indices != -1]  # filter out -1s\n",
    "    out[key, valid_indices] += 1  # increment count for those indices\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 2,\n",
       "         2, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 2, 2, 2, 4, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 2, 2, 4, 2, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [0, 2, 3, 4, -1, 6, -1, -1, -1],\n",
    "    [1, 0, 1, -1, -1, -1, 5, 6, 7],\n",
    "], dtype=torch.long)\n",
    "\n",
    "embedding_indices = x[:, 0].long()        # Shape: (n,)\n",
    "indices = x[:, 1:]  # shape: (n, 8)\n",
    "\n",
    "# Create mask for valid (non -1) indices\n",
    "mask = indices != -1\n",
    "\n",
    "# Flatten valid keys and corresponding indices\n",
    "flat_keys = keys.unsqueeze(1).expand_as(indices)[mask]     # shape: (num_valid,)\n",
    "flat_indices = indices[mask]                               # shape: (num_valid,)\n",
    "ones = torch.ones_like(flat_indices, dtype=torch.long)\n",
    "\n",
    "# Use scatter_add to add ones at (flat_keys, flat_indices)\n",
    "out.index_put_((flat_keys, flat_indices), ones, accumulate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 4, 4, 4, 0, 3, 5],\n",
      "        [1, 2, 3, 0, 3, 5, 4, 4, 4]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example vertex list to connect to vn\n",
    "vertex_list = torch.tensor([0, 3, 5])  # indices of existing nodes\n",
    "\n",
    "# Original edge_index\n",
    "edge_index = torch.tensor([\n",
    "    [0, 1, 2],\n",
    "    [1, 2, 3]\n",
    "], dtype=torch.long)  # shape [2, num_edges]\n",
    "\n",
    "# Define the virtual node index\n",
    "vn = edge_index.max().item() + 1  # or simply num_nodes if known\n",
    "\n",
    "# Create new edges: from vn to each vertex\n",
    "vn_to_v = torch.stack([torch.full_like(vertex_list, vn), vertex_list], dim=0)\n",
    "\n",
    "# (Optional) also add reverse edges if undirected or bidirectional\n",
    "v_to_vn = torch.stack([vertex_list, torch.full_like(vertex_list, vn)], dim=0)\n",
    "\n",
    "# Combine\n",
    "new_edges = torch.cat([vn_to_v, v_to_vn], dim=1)  # shape [2, 2 * n]\n",
    "edge_index = torch.cat([edge_index, new_edges], dim=1)\n",
    "\n",
    "print(edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New edges:\n",
      " tensor([[10, 10, 10, 11, 11,  0,  3,  5,  2,  4],\n",
      "        [ 0,  3,  5,  2,  4, 10, 10, 10, 11, 11]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example inputs\n",
    "vertex_lists = torch.tensor([\n",
    "    [0, 3, 5, -1],\n",
    "    [2, 4, -1, -1]\n",
    "])  # shape: [num_vns, max_neighbors]\n",
    "\n",
    "vn_ids = torch.tensor([10, 11])  # virtual node IDs\n",
    "\n",
    "# 1. Mask out the padded entries (-1)\n",
    "mask = vertex_lists != -1  # shape: [num_vns, max_neighbors]\n",
    "\n",
    "# 2. Prepare VN -> vertex edges\n",
    "vn_repeat = vn_ids.unsqueeze(1).expand_as(vertex_lists)  # [num_vns, max_neighbors]\n",
    "\n",
    "src = vn_repeat[mask]     # virtual node source\n",
    "dst = vertex_lists[mask]  # real node targets\n",
    "\n",
    "vn_to_v = torch.stack([src, dst], dim=0)\n",
    "\n",
    "# 3. (Optional) reverse direction\n",
    "v_to_vn = torch.stack([dst, src], dim=0)\n",
    "\n",
    "# 4. Combine all new edges\n",
    "new_edges = torch.cat([vn_to_v, v_to_vn], dim=1)  # shape [2, num_edges]\n",
    "\n",
    "print(\"New edges:\\n\", new_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 10, 10, 11, 11])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 3, 5, 2, 4])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from ogb.graphproppred import PygGraphPropPredDataset\n",
    "from torch_geometric.datasets import ZINC\n",
    "import tqdm\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.datasets import PCQM4Mv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://dgl-data.s3-accelerate.amazonaws.com/dataset/OGB-LSC/pcqm4m-v2.zip\n",
      "Extracting data/pcqm4m-v2/raw/pcqm4m-v2.zip\n",
      "Processing...\n",
      "100%|██████████| 3747/3747 [00:15<00:00, 248.64it/s]\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(PCQM4Mv2(3378606), 3747)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = PCQM4Mv2(root='./data/pcqm4m-v2', split='train')\n",
    "dataset, dataset.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=5.8831014478100006, smiles='CN1CCN([C@H]2[C@@H]1C(=CC2)C(C)C)C')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6, 0, 4, 5, 3, 0, 4, 0, 0],\n",
       "        [7, 0, 3, 5, 0, 0, 4, 0, 1],\n",
       "        [6, 0, 4, 5, 2, 0, 4, 0, 1],\n",
       "        [6, 0, 4, 5, 2, 0, 4, 0, 1],\n",
       "        [7, 0, 3, 5, 0, 0, 4, 0, 1],\n",
       "        [6, 1, 4, 5, 1, 0, 4, 0, 1],\n",
       "        [6, 2, 4, 5, 1, 0, 4, 0, 1],\n",
       "        [6, 0, 3, 5, 0, 0, 3, 0, 1],\n",
       "        [6, 0, 3, 5, 1, 0, 3, 0, 1],\n",
       "        [6, 0, 4, 5, 2, 0, 4, 0, 1],\n",
       "        [6, 0, 4, 5, 1, 0, 4, 0, 0],\n",
       "        [6, 0, 4, 5, 3, 0, 4, 0, 0],\n",
       "        [6, 0, 4, 5, 3, 0, 4, 0, 0],\n",
       "        [6, 0, 4, 5, 3, 0, 4, 0, 0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tokengt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
